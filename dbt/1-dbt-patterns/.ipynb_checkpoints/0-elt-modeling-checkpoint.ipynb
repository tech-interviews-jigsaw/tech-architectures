{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fresh-builder",
   "metadata": {},
   "source": [
    "# Modeling in DBT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-vaccine",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-kentucky",
   "metadata": {},
   "source": [
    "Now that we have gotten our DBT account connected to our data warehouse and our github repository, it's time to use DBT to perform some modeling.  Modeling means transforming  our data from raw data to other tables that make the data easier to work with.  In an ELT paradigm, we directly load raw data into our database without DBT, and then use DBT to transform the data from there.  \n",
    "\n",
    "In this lesson, we'll learn more about this structure for transforming our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-ordering",
   "metadata": {},
   "source": [
    "### Our Starting Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-bibliography",
   "metadata": {},
   "source": [
    "Now with the ELT paradigm, we essentially begin with a data dump.  This is what occurs with the extract and load steps in ELT.  We simply take data from either an internal data source, or external APIs, and dump it into our data warehouse.\n",
    "\n",
    "For example, take a look at the diagram below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3a852-9d9d-4276-a908-5ccf28dcca36",
   "metadata": {},
   "source": [
    "> <img src=\"./rittman-pipeline.png\" width=\"80%\">\n",
    "\n",
    "> Diagram courtesy of [Rittman analytics](https://github.com/rittmananalytics/ra_data_warehouse)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e93b6-db31-4e6b-9a0a-b659d2f3b6b2",
   "metadata": {},
   "source": [
    "Above we can see that data comes from various sources, and that DBT is used to transform the data to ultimately feed to dashboards or send formatted data to various departments.  Each one of those dashboards is a datamart, consisting of fact and dimension tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-collective",
   "metadata": {},
   "source": [
    "> <img src=\"./star_pagila.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-paris",
   "metadata": {},
   "source": [
    "So we'll use DBT to transform our source data to data for our data marts.  Remember, that we'll start with our data dumped into our data warehouse (eg. via Fivetran or manually), and from there we can perform transformations to go from the source data to data marts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-athens",
   "metadata": {},
   "source": [
    "### The transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a3ee7-3f42-4fd4-b95c-048fced444c6",
   "metadata": {},
   "source": [
    "Now let's take a look at the process for making these transformations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ee9c5-1926-48a2-b4e8-cd0c55df703b",
   "metadata": {},
   "source": [
    "<img src=\"./dbt-pipeline.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1a890-a116-420b-ac8b-a0481a2cd552",
   "metadata": {},
   "source": [
    "In the diagram above, we can see that we start with our **source tables**, which is our raw data, then clean up that raw data with **staging**, combine data from different sources in **integration**, and finally organize the data in our star schema for stakeholders in the mart layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf1be47-f054-48f6-8834-49fb0a1cac6f",
   "metadata": {},
   "source": [
    "Let's discuss this in a bit more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-dancing",
   "metadata": {},
   "source": [
    "1. The source tables\n",
    "\n",
    "We call these tables that house our untransformed data our **source tables**. These are the tables from our data dump.  Some of our source tables will come from our OLTP database, but many will come from third party sources.  There isn't a lot of work to be do, at this layer, as these tables consist simply of our raw data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd34c76-fe47-4084-be9f-27445e828245",
   "metadata": {},
   "source": [
    "2. Staging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-interval",
   "metadata": {},
   "source": [
    "Now as we know, when we first load our data, there will be transformations that we will want to apply to each of these tables.  The idea with staging, is to use views to clean up each of our source tables, but not to combine any of our data at this point. \n",
    "\n",
    "Many of the transformations that we apply would be light transformations.  For example, in the diagram below we start with `customers` source table drawn from hubspot data, and create a new table with the same data and changing the column names of `first_name`, `last_name` `address_id` to `first`, `last`, `address_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-laser",
   "metadata": {},
   "source": [
    "> <img src=\"./customers_stg.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-danish",
   "metadata": {},
   "source": [
    "> Notice that all of the data in a single staging table is derived from a single source table (just like we see with our customers table above).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-toronto",
   "metadata": {},
   "source": [
    "2. Staging to Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecac033-7908-4784-95f8-b8e1904e6698",
   "metadata": {},
   "source": [
    "Now take another look at what occurs from the staging to integration layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597cb4e0-160e-4c7e-81e6-5fcc13282565",
   "metadata": {},
   "source": [
    "<img src=\"./integration.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5fe60b-0d6c-404c-b8e3-f2cc17ebb678",
   "metadata": {},
   "source": [
    "When we move from staging to integration, we combine data from multiple tables.  For example, we may be looking to combine user information from hubspot, stripe, and mailchimp into one users table.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00485b-5a92-42a5-93da-20f501c156ea",
   "metadata": {},
   "source": [
    "3. Mart tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db57b0-3281-4322-8b14-31b528363dae",
   "metadata": {},
   "source": [
    "Finally, with our mart data we have tables structured for various stakeholders.  These are organized with fact and dimension tables to for data dashboards or to provide directly to various stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-tuesday",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-accommodation",
   "metadata": {},
   "source": [
    "In this lesson, we saw some of the workflow and naming conventions in our data modeling process.  We start with our data already loaded into our data warehouse in our **source tables**.  Then, we use **staging tables** to apply transformations to our source tables.  Each staging table should only reference in a single source table.  Then we saw how we can use the integration layer to combine our data.  Finally, we use the staging tables to create our **mart tables**, which is the name given to our dimension and fact tables.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
