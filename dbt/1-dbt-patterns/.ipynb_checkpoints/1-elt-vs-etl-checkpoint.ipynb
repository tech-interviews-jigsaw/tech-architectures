{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "damaged-career",
   "metadata": {},
   "source": [
    "# Modeling in DBT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-finish",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-advancement",
   "metadata": {},
   "source": [
    "Now that we have gotten our DBT account connected to our data warehouse and our github repository, it's time to use DBT to perform some modeling.  Modeling means transforming  our data from raw data to other tables that make the date easier to work with.  In an ELT paradigm, we directly load raw data into our database without DBT, and then use DBT to transform the data from there.  \n",
    "\n",
    "In this lesson, we'll learn more about this structure for transforming our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-wedding",
   "metadata": {},
   "source": [
    "### ETL to ELT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-psychology",
   "metadata": {},
   "source": [
    "Now, when thinking about the transformations that occur with DBT, our pattern is similar to the pattern we saw with ETL.  With ETL, we retrieve data from an API resource -- like amazon products -- then write an adapter or builder to that selects and coerces the data as needed, and finally inserts that data into a database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-hamilton",
   "metadata": {},
   "source": [
    "> Take a look at our ETL pattern below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-branch",
   "metadata": {},
   "source": [
    "<img src=\"./etl_process.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-priority",
   "metadata": {},
   "source": [
    "> So above in the **ETL process**, we request data from an external API, transform the JSON, and then insert it into the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-appreciation",
   "metadata": {},
   "source": [
    "Now, when we move to the *extract load and then transform* pattern with DBT, we undergo a similar process as we saw with ETL.  \n",
    "\n",
    "The difference of course, is that we extract and then immediately load our data into the database.  And because this data is just the raw source data from our API, we then transform this data again into one of our OLAP tables that has our data more cleaned up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-hamburg",
   "metadata": {},
   "source": [
    "<img src=\"./elt_paradigm.jpg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-neighbor",
   "metadata": {},
   "source": [
    "> So we can see above, that we immediately load our data into the database.  And then we load our data from the database, to transform it and load it into a either a fact or dimension table like, `dim_products`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-novel",
   "metadata": {},
   "source": [
    "One thing to note about our ELT diagram, is that we are performing this entire process in our data warehouse.  So each one of those tables, `source_products`, and `dim_products` lives in our warehouse, and not our transactional database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-technique",
   "metadata": {},
   "source": [
    "* Moving to DBT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-edwards",
   "metadata": {},
   "source": [
    "So above, we saw how our process varies a bit when moving to our Extract Load Transform paradigm.  Now, when we think about the DBT in this process, just remember that DBT only handles the *transformation process* of ELT.  By the time DBT handles our data, it's already loaded into our data warehouse.  \n",
    "\n",
    "> The light blue rectangles indicates the where DBT is involved.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-acting",
   "metadata": {},
   "source": [
    "<img src=\"./dbt_eco_1.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-dimension",
   "metadata": {},
   "source": [
    "So really, with DBT, to create a fact table we query our source table to extract the data and columns we need for our fact or dimension tables.  This is exactly what we saw in previous lessons.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-friendship",
   "metadata": {},
   "source": [
    "> For example, when working with the categories table, we started with a categories table that had columns of `catid`, `catgroup`, `catname`, and `catdescription`.  And from this, we selected just the first three columns and renamed them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-round",
   "metadata": {},
   "source": [
    "> <img src=\"./cat_results.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-combining",
   "metadata": {},
   "source": [
    "The take away, is that we should think of this process as being similar to our ETL pattern.  In that ETL pattern, we started with messy data from the external data source, and then cleaned it up.  And in this ELT pattern we also start with messy data from an external data source, but we still load that messy data into a **source table** and then we query that source table to clean up our data and insert it into a fact or dimension table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-interview",
   "metadata": {},
   "source": [
    "### Why ELT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-korean",
   "metadata": {},
   "source": [
    "So we just saw a move from an ETL paradigm to an ELT paradigm.  What are some of the benefits to the change?\n",
    "\n",
    "1. With ELT, we store *all* of the data.  So we hold onto external data, and this way, we don't potentially lose data that we may find valuable later on.\n",
    "\n",
    "2. Another benefit is that once our data is stored in the database, we *just need SQL*.  So any team members that do not know how access and manipulate data with Python, or insert data in postgres or AWS do not really need to.  All of the data is already in there, and they just need to write select statements to access it.\n",
    "\n",
    "A downside of ELT could be simply the amount of data that is stored, and the costs associated with that.  However, with storage costs decreasing, more companies are moving to the ELT pattern.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-chapter",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-excuse",
   "metadata": {},
   "source": [
    "In this lesson, we learned about the ELT pattern with DBT.  Here, we saw that extract our data from an external API, and immediately load that messy data into a **source table**.  From there, we extract the data from the source data and insert it into an **fact** or **dimension table**.  The fact or dimension tables have cleaned up data that we can use to then perform queries and transformations to deliver to various stakeholders or connect to a dashboard tool like looker or tableau. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-permit",
   "metadata": {},
   "source": [
    "<img src=\"./dbt_eco_1.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-metabolism",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
